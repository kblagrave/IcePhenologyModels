{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT**: \n",
    "- topUSlakes.csv : ice phenology summary for 100 US lakes\n",
    "    - columns include:\n",
    "        - lakecode: unique identifier for lake\n",
    "        - lake: lake name\n",
    "        - start_date: first year of ice record\n",
    "        - end_date: last year of ice record\n",
    "        - lat: latitude \n",
    "        - lon: longitude\n",
    "        - Elevation: elevation (m)\n",
    "\n",
    "\n",
    "- topUSlakes_weather_stations: filtered list of NOAA weather stations within 50 km of lakes\n",
    "    - the complete list of stations, f'mshr_enhanced_202102.txt' is available online from https://www.ncei.noaa.gov/access/homr/reports/mshr\n",
    "- weather station data: a separate csv file for each weather station\n",
    "    - saved in WEATHER_DIR\n",
    "    - filename should be [GHCND_ID].csv (e.g., US1MNBW0013.csv)\n",
    "    - these data can be downloaded from https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries\n",
    "        - API documentation: https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation\n",
    "\n",
    "**PARAMETERS**:\n",
    "- delta_elevation = 100 (maximum allowed elevation difference between lake and weather station)\n",
    "\n",
    "**OUTPUT**: \n",
    "- top100_extra_weather_all_deltaElev_100m.csv\n",
    "\n",
    "    - columns include:\n",
    "        - DATE\n",
    "        - lakecode\n",
    "        - lake\n",
    "        - STATION\n",
    "        - TMINMAX\n",
    "        - SNOW\n",
    "        - PRCP\n",
    "        - SNWD\n",
    "        - TSUN\n",
    "        \n",
    "        \n",
    "**DEPENDENCIES**:\n",
    "- ``kpmb_weather.py`` is found in `modules` directory (requirements: geopy, pandas, requests)\n",
    "- pandas, matplotlib, IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from kpmb_weather import find_stations, get_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ICEMODELS_DIR, the directory where the ice phenology data are saved.\n",
    "\n",
    "Define WEATHER_DIR, the directory where the downloaded weather csv files are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = 'My Passport for Mac'\n",
    "\n",
    "ICEMODELS_DIR = Path(f'/Volumes/{volume}/IceModels/')\n",
    "WEATHER_DIR = Path(f'/Volumes/{volume}/WeatherData/NOAA/csv/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for weather station search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_elev_m = 100 # maximum allowed difference between lake and weather station elevation\n",
    "\n",
    "delta_elev_ft = delta_elev_m*3.28084 # 100 metres converted to feet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in csv containing ice phenology data for lakes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlakes = 100\n",
    "\n",
    "dftopN = pd.read_csv(ICEMODELS_DIR/'topUSlakes.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in list of all stations within 50 km of lake coordinate. The file ``topUSlakes_weather_stations.csv`` was created from ``mshr_enhanced_202102.txt`` (https://www.ncei.noaa.gov/access/homr/reports/mshr)\n",
    "\n",
    "Important columns include:\n",
    "- BEGIN_DATE and END_DATE\n",
    "- NCDCSTN_ID\n",
    "- latitude and longitude of station\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstations = pd.read_csv(ICEMODELS_DIR/'topUSlakes_weather_stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for and remove stations that do not contain any temperature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(dfstations.GHCND_ID.unique()):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1:4d}/{len(dfstations.GHCND_ID.unique())}\")\n",
    "    try:\n",
    "        df_ = pd.read_csv(WEATHER_DIR/'{c}.csv')\n",
    "        # look for stations that are missing temperature data\n",
    "        if len(df_.dropna(how='all',axis=1).columns.intersection(['TMIN','TMAX','TAVG']))==0:\n",
    "            dfstations.loc[dfstations.GHCND_ID==c,'temperature'] = False\n",
    "        else:\n",
    "            dfstations.loc[dfstations.GHCND_ID==c,'temperature'] = True\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select list of stations that have temperature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstations_temp = dfstations[dfstations.temperature==True]\n",
    "#dfstations_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build meteorological time series\n",
    "\n",
    "Build weather time series based on availability of TMINMAX (i.e., TMIN, TMAX and TAVG data).\n",
    "- **NEW FEB 10 2022** All data in a given row is from the same weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfworking = pd.DataFrame()\n",
    "#dfworking_station = pd.DataFrame()\n",
    "\n",
    "# run through list of lakes\n",
    "for i,row in dftopN.iterrows():\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1:2d}/100 - {row.lakecode}\")\n",
    "    lat,lon = row.lat, row.lon\n",
    "\n",
    "    # elevation of lake\n",
    "    elevation = row.Elevation *3.28084 # convert to feet since stations are in feet\n",
    "\n",
    "    # row.start_date and row.end_date are the years for which we have ice phenology records\n",
    "    #   need weather data for this same period\n",
    "    #\n",
    "    start_date = f'1 January {row.start_date}'\n",
    "    end_date = f'31 December {row.end_date}'\n",
    "\n",
    "    # finds all weather stations within 50 km of lat, lon\n",
    "    #    with data between start_date and end_date,\n",
    "    #    with a maximum delta elevation of 100 m.\n",
    "    stations,_,distances = find_stations(lat,lon,dfstations_temp,\n",
    "                                         verbose=False,\n",
    "                      dist=50,elev=elevation, d_elev=delta_elev_ft,\n",
    "                      start_date=start_date,end_date=end_date)\n",
    "    \n",
    "    # stations is a dictionary of {station_number: [list of timestamps]}\n",
    "    # where station_number is NCDCSTN_ID\n",
    "    #   - order is from station with most coverage to station with the least coverage\n",
    "    #         within the date range specified\n",
    "    \n",
    "    # get list of station ID from closest to furthest\n",
    "    other_stations = get_id(lat,lon, dfstations_temp, dist=50,elev=elevation,\n",
    "                            d_elev=delta_elev_ft,getid='GHCND_ID')\n",
    "\n",
    "    # start to build up time series, starting with an empty data frame\n",
    "    #   which covers the entire range of the ice phenology record from \n",
    "    #   start_date to end_date\n",
    "    df_lakecode = pd.DataFrame(index=pd.date_range(start_date,end_date))\n",
    "    \n",
    "    # CAVEAT: cycles from most to least coverage (but no specific mention of TMIN or TMAX) \n",
    "    #     so could be most SNOW coverage perhaps\n",
    "    for key,value in stations.items():\n",
    "        # key is station number NCDCSTN_ID\n",
    "        # value is list of dates\n",
    "        # order of stations is from most to least coverage\n",
    "        \n",
    "        #d1min = min(d1).strftime('%Y-%m-%d')\n",
    "        #d1max = max(d1).strftime('%Y-%m-%d')\n",
    "        # get list of GHCND_ID that match NCDCSTN_ID\n",
    "        ghcnd_id = dfstations_temp[dfstations_temp.NCDCSTN_ID==key].GHCND_ID.dropna().unique()\n",
    "        \n",
    "        # cycle through all stations associated with this NCDCSTN_ID (key)\n",
    "        for ghcnd in ghcnd_id:\n",
    "            # read in data from file (all data gets read in)\n",
    "            filename = WEATHER_DIR/f'{ghcnd}.csv'\n",
    "            try:\n",
    "                dfoo = pd.read_csv(filename)\n",
    "            except:\n",
    "                print('file not found error.')\n",
    "                continue\n",
    "            dfoo = dfoo.set_index('DATE')\n",
    "            dfoo.index = pd.to_datetime(dfoo.index)\n",
    "            \n",
    "            # check quality attributes and remove bad data\n",
    "            for var in ['TMIN','TMAX','TAVG']:\n",
    "                dfoo[var] = dfoo[var].astype(float)\n",
    "                ind_flag = ~dfoo[f'{var}_ATTRIBUTES'].astype(str).str.split(',').str[1].astype(str).str.strip().isin(['','nan'])\n",
    "                dfoo.loc[ind_flag,var] = np.nan\n",
    "            \n",
    "            # calculate average temperature from TMIN and TMAX\n",
    "            dfoo['TMINMAX'] = (dfoo['TMIN'] + dfoo['TMAX'])/2.\n",
    "            \n",
    "            # if TMIN and TMAX do not exist (i.e., they are NaN) replace with TAVG value\n",
    "            ind = dfoo.TMINMAX.isnull() & ~dfoo.TAVG.isnull()\n",
    "            dfoo.loc[ind,'TMINMAX'] = dfoo.loc[ind,'TAVG'].copy()\n",
    "            \n",
    "            station = dfoo.STATION.drop_duplicates().tolist()[0]\n",
    "            \n",
    "            # fills in blanks with data from other stations\n",
    "            \n",
    "            if df_lakecode.shape[1]==0:\n",
    "                df_lakecode = df_lakecode.merge(dfoo,left_index=True,right_index=True, how='left')\n",
    "                #df_station_record = df_lakecode.astype(str).copy()\n",
    "                #df_station_record[df_station_record!='nan'] = station\n",
    "            \n",
    "                df_lakecode['TMINMAX'] = (df_lakecode['TMIN']+ df_lakecode['TMAX'])/2.\n",
    "                ind = df_lakecode.TMINMAX.isnull() & ~df_lakecode.TAVG.isnull()\n",
    "                df_lakecode.loc[ind,'TMINMAX'] = df_lakecode.loc[ind,'TAVG'].copy()\n",
    "            \n",
    "            else:\n",
    "                # [old version]\n",
    "                # df_lakecode= df_lakecode.fillna(dfoo)\n",
    "                \n",
    "                # fill in where TMINMAX is null\n",
    "                ind_rows = df_lakecode.TMINMAX.isnull() & ~dfoo.TMINMAX.isnull()\n",
    "            \n",
    "                df_lakecode.loc[ind_rows,:] = dfoo.loc[ind_rows,:].copy()\n",
    "                \n",
    "                #ind_new = df_lakecode.isnull() & ~dfoo.isnull() #& (df_station_record=='nan')\n",
    "                #df_station_record[ind_new] = station\n",
    "                #\n",
    "                \n",
    "    # now fillna using other_stations    \n",
    "    for ghcnd in other_stations:\n",
    "        #print(ghcnd)\n",
    "        filename = WEATHER_DIR/f'{ghcnd}.csv'\n",
    "        try:\n",
    "            dfoo = pd.read_csv(filename,low_memory=False)\n",
    "        \n",
    "        except:\n",
    "            print(ghcnd)\n",
    "            print('file not found error.')\n",
    "            continue\n",
    "        if len(dfoo)==0:\n",
    "            continue\n",
    "            \n",
    "        #print(ghcnd, dfoo.DATE.min(),dfoo.DATE.max())\n",
    "        dfoo = dfoo.set_index('DATE')\n",
    "        dfoo.index = pd.to_datetime(dfoo.index)\n",
    "        \n",
    "        # check quality attributes and remove bad data\n",
    "        for var in ['TMIN','TMAX','TAVG']:\n",
    "            dfoo[var] = dfoo[var].astype(float)\n",
    "            ind_flag = ~dfoo[f'{var}_ATTRIBUTES'].astype(str).str.split(',').str[1].astype(str).str.strip().isin(['','nan'])\n",
    "            dfoo.loc[ind_flag,var] = np.nan\n",
    "\n",
    "        dfoo['TMINMAX'] = (dfoo['TMIN'] + dfoo['TMAX'])/2.\n",
    "        ind = dfoo.TMINMAX.isnull() & ~dfoo.TAVG.isnull()\n",
    "        dfoo.loc[ind,'TMINMAX'] = dfoo.loc[ind,'TAVG'].copy()\n",
    "        \n",
    "        #station = dfoo.STATION.drop_duplicates().tolist()[0]\n",
    "        if df_lakecode.shape[1]==0:\n",
    "            df_lakecode = df_lakecode.merge(dfoo,left_index=True,right_index=True, how='left')\n",
    "            \n",
    "            df_lakecode['TMINMAX'] = (df_lakecode['TMIN']+ df_lakecode['TMAX'])/2.\n",
    "            ind = df_lakecode.TMINMAX.isnull() & ~df_lakecode.TAVG.isnull()\n",
    "            df_lakecode.loc[ind,'TMINMAX'] = df_lakecode.loc[ind,'TAVG'].copy()\n",
    "            \n",
    "            #df_station_record = df_lakecode.astype(str).copy()\n",
    "            #df_station_record[df_station_record!='nan'] = station\n",
    "        else:\n",
    "            # [old version] filling blanks with data from different stations; \n",
    "            # so SNOW and TMINMAX for example may be from\n",
    "            #     different stations... (record station!)\n",
    "            # df_lakecode= df_lakecode.fillna(dfoo)\n",
    "            \n",
    "            # consistently fill in data only if TMINMAX is missing\n",
    "            ind_rows = df_lakecode.TMINMAX.isnull() & ~dfoo.TMINMAX.isnull()\n",
    "            \n",
    "            df_lakecode.loc[ind_rows,:] = dfoo.loc[ind_rows,:].copy()\n",
    "            \n",
    "    #df_station_record['lakecode']= row.lakecode\n",
    "    #df_station_record['lake'] = row.lake\n",
    "    #df_station_record = df_station_record.reset_index().rename({'index':'DATE'},axis=1)\n",
    "    #dfworking_station = dfworking_station.append(df_station_record,ignore_index=True)\n",
    "    \n",
    "    df_lakecode['lakecode']= row.lakecode\n",
    "    df_lakecode['lake'] = row.lake\n",
    "    df_lakecode = df_lakecode.reset_index().rename({'index':'DATE'},axis=1)\n",
    "    \n",
    "    dfworking = pd.concat([dfworking, df_lakecode],ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of lakes and stations in meteorological time series\n",
    "Look at all time series, recording the occurrence of use (in days) for each of the weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfworking.groupby(['lakecode','STATION']).DATE.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfworking.set_index('DATE').SNOW.plot()\n",
    "dfworking.STATION.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA/QC of meteorological time series\n",
    "\n",
    "List of variables:\n",
    "\n",
    "- (AWDR : wind direction)\n",
    "- AWND (m/s) : average daily wind speed\n",
    "- PRCP (mm): precipitation\n",
    "- SNOW (mm): snow\n",
    "- SNWD (mm): snow depth\n",
    "- TAVG (C): average temperature?\n",
    "- THIC (mm): thickness of ice on water\n",
    "- TMAX (C): maximum temperature\n",
    "- TMIN (C): minimum temperature\n",
    "- TOBS (C): temperature at the time of observation\n",
    "- TSUN (minutes): daily total sunshine\n",
    "\n",
    "#### 1. Check attribute columns (Meas,Qual,Source,Time) and remove data with a non-blank Qual flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy list of all stations and daily meteorological data to dfresult\n",
    "dfresult = dfworking.copy()\n",
    "\n",
    "for c in dfworking.columns:\n",
    "    if 'ATTRIBUTES' in c:\n",
    "        if len(dfworking[c].dropna())>0:\n",
    "            new_c = f\"{c}_QUALITY\"\n",
    "            print(new_c)\n",
    "            \n",
    "            # Add new columns containing data quality flags only\n",
    "            dfresult[new_c] = dfworking[c].dropna().str.split(',').str[1]\n",
    "            dfresult[new_c] = dfresult[new_c].str.strip().replace('',np.nan)\n",
    "            \n",
    "            # Remove flagged data from table\n",
    "            ind = ~dfresult[new_c].isnull()\n",
    "            dfresult.loc[ind,c.replace('_ATTRIBUTES','')] = np.nan\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert columns to float dtype, date and sort by lakecode and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in dfresult.columns:\n",
    "    try:\n",
    "        dfresult[c] = dfresult[c].astype(float)\n",
    "    except:\n",
    "        print(c)\n",
    "\n",
    "dfresult.DATE = pd.to_datetime(dfresult.DATE)\n",
    "\n",
    "dfresult = dfresult.sort_values(['lakecode','DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-calculate daily average temperatures\n",
    "\n",
    "\n",
    "$$T_{\\rm avg} = \\frac{T_{\\rm min}+T_{\\rm max}}{2}$$\n",
    "\n",
    "\n",
    "- fill in missing (or low quality) data with TAVG, if the value exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult['TMINMAX'] = (dfresult.TMIN + dfresult.TMAX)/2.\n",
    "\n",
    "# confirm that average is null where it should be\n",
    "ind = dfresult.TMIN.isnull() | dfresult.TMAX.isnull() \n",
    "\n",
    "display(dfresult.loc[ind,'TMINMAX'].dropna())\n",
    "\n",
    "dfresult.loc[ind,'TMINMAX'] = np.nan\n",
    "\n",
    "# fill in blanks with TAVG if it exists\n",
    "\n",
    "ind = dfresult.TMINMAX.isnull() & ~dfresult.TAVG.isnull()\n",
    "\n",
    "dfresult.loc[ind,'TMINMAX'] = dfresult.loc[ind,'TAVG']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Look at snow and precipitation\n",
    "\n",
    "a) How are snow and precipitation related?\n",
    "- it seems that they are independent\n",
    "- PRCP is \"rain, melted snow\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.plot(x='PRCP',y='SNOW',ls='none',marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Check that it is cold enough for snow.\n",
    "- looking at snowfall when average temperature is above 0 degrees C.\n",
    "- when there is a warmer average daily temperature, there is less snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = (dfresult.SNOW>0) & (dfresult.TMINMAX>0)\n",
    "\n",
    "x = dfresult[ind].SNOW\n",
    "y = dfresult[ind].TMINMAX\n",
    "plt.plot(x,y, ls='none',marker='.')\n",
    "plt.ylabel('Temperature (\\N{DEGREE SIGN}C)')\n",
    "plt.xlabel('Amount of snow fall (mm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = (dfresult.TMINMAX>15) & (dfresult.SNOW > 0)\n",
    "#dfresult[ind].dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate missing/flagged data in meteorological time series\n",
    "\n",
    "Fill in blanks. Linearly interpolate over blank days, up to 3 consecutive NaN.\n",
    "\n",
    "- should be OK for temperature, snow depth\n",
    "- precipitation and snow: may not make sense to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult_filled = dfresult.copy()\n",
    "\n",
    "# run through each lake's timeseries\n",
    "for name,group in dfresult[['DATE','STATION','lakecode','lake','TMINMAX','SNOW','PRCP','SNWD','TSUN']].groupby('lakecode'):\n",
    "    clear_output(wait=True)\n",
    "    print(name)\n",
    "    # cycle through TMINMAX, SNWD and TSUN, interpolating each\n",
    "    for c in ['TMINMAX',\n",
    "              #'SNOW','PRCP', # can't justify interpolating snow and precipitation\n",
    "              'SNWD', # snow depth would make sense to interpolate between two days.\n",
    "              'TSUN' # number of sun minutes can also be justified for interpolation\n",
    "             ]:\n",
    "        ts = group[c].astype(float)\n",
    "        \n",
    "        ts_filled = ts.interpolate(method='linear',limit=3, limit_area='inside')\n",
    "        ind = (ts!=ts_filled) & (~ts_filled.isnull())\n",
    "        print(c,ind.sum())\n",
    "        # replace old time series with this new interpolated time series\n",
    "        dfresult_filled.loc[group.index,c] = ts_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfresult[dfresult.lakecode=='DMR2'].set_index('DATE').tail(8380).head(10).dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some lakes still have incomplete meteorological time series\n",
    "Interpolation may not completely fill the meteorological record.\n",
    "\n",
    "These lakes have incomplete daily temperature records in the period from 1960 to present. Incomplete years are listed after the lakecode and lake name.\n",
    "\n",
    "All other lakes have complete temperature records in this 1960-present time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,group in dfresult_filled[dfresult_filled.TMINMAX.isnull() & (dfresult_filled.DATE.dt.year>=1960)].groupby('lakecode'):\n",
    "    print(name, group.lake.drop_duplicates().to_list()[0], group.DATE.dt.year.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check completion of columns\n",
    "#ind = dfresult_filled.TMINMAX.isnull() #& ~dfresult_filled.TOBS.isnull()\n",
    "#display(dfresult_filled[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save continuous time series\n",
    "- changed to \"extra\" weather in filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult_filled.to_csv(ICEMODELS_DIR/f'top{nlakes}_extra_weather_all_deltaElev_{delta_elev_m}m.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Extra QA/QC: Check weather station elevation vs lake elevation\n",
    "All lakes should be within 100 m of the stations used to build their daily meteorological time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult = pd.DataFrame()\n",
    "\n",
    "for stn,group in dfresult_filled.groupby('STATION'):\n",
    "    dfmerge = group.copy()\n",
    "    \n",
    "    dfstn = dfstations.loc[dfstations.GHCND_ID==stn,['BEGIN_DATE','END_DATE','GHCND_ID','ELEV_GROUND','ELEV_GROUND_UNIT']]\n",
    "\n",
    "    # backfill/forward fill if data is missing for given years, assuming elevation hasn't changed\n",
    "    dfstn = dfstn.fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    dfstn.BEGIN_DATE = dfstn.BEGIN_DATE.replace(10101,17000101)\n",
    "    dfstn.END_DATE = dfstn.END_DATE.replace(99991231,21001231)\n",
    "    \n",
    "    newindex = pd.to_datetime(dfstn.set_index('BEGIN_DATE').index.tolist(),format='%Y%m%d')\n",
    "    dfstn.index = newindex\n",
    "    \n",
    "    # create a continuous time series\n",
    "    dfstn_new = dfstn[~dfstn.index.duplicated(keep='first')]\n",
    "    dfstn_new = dfstn_new.resample('D').ffill()\n",
    "    # find where elevation of station changes\n",
    "    dfstn_new = dfstn_new[dfstn_new.ELEV_GROUND.diff() != 0]\n",
    "    # redefine END_DATE based on when elevation changes (i.e., day before next BEGIN_DATE)\n",
    "    dfstn_new['END_DATE'] = dfstn_new.index #-pd.to_timedelta('1 day')\n",
    "    dfstn_new['END_DATE'] = dfstn_new['END_DATE'].shift(-1).dt.strftime('%Y%m%d')\n",
    "    dfstn_new = dfstn_new.reset_index(drop=True)\n",
    "    \n",
    "    # make sure all dates before and after are included in case the station summary file is missing dates\n",
    "    dfstn_new.iloc[0,0] = 10101\n",
    "    dfstn_new.iloc[-1,1] = 99991231\n",
    "    dfstn = dfstn_new.copy()\n",
    "    \n",
    "    if len(dfstn)==1:\n",
    "        dfoo = group.reset_index().merge(dfstn,left_on='STATION',right_on='GHCND_ID').set_index('index')\n",
    "    elif len(dfstn)==0:\n",
    "        print('missing')\n",
    "        break\n",
    "    else:\n",
    "        df_elev = pd.DataFrame()\n",
    "        for i,row in dfstn.iterrows():\n",
    "            if row.BEGIN_DATE==10101:\n",
    "                #display(dfstn)\n",
    "                begin_date = pd.to_datetime('1700-01-01')\n",
    "            else:\n",
    "                begin_date = pd.to_datetime(row.BEGIN_DATE, format='%Y%m%d')\n",
    "            if row.END_DATE==99991231:\n",
    "                end_date = pd.to_datetime('2100-12-31')\n",
    "            else:\n",
    "                # subtract one day so inclusive between statement below works as expected\n",
    "                end_date = pd.to_datetime(row.END_DATE, format='%Y%m%d') - pd.to_timedelta('1 day')\n",
    "            #print(begin_date,end_date)\n",
    "            ind = group.DATE.between(begin_date, end_date, inclusive='both')\n",
    "            solution_ =  group[ind].reset_index().merge(row.to_frame().T, left_on='STATION',right_on='GHCND_ID',how='left').set_index('index')\n",
    "            #df_elev = df_elev.append(solution_[row.index].dropna())\n",
    "            \n",
    "            df_elev = pd.concat([df_elev, solution_[row.index].dropna()])\n",
    "            \n",
    "            #if len(df_elev) == len(group):\n",
    "            #    break\n",
    "                \n",
    "        try:\n",
    "            assert len(df_elev) == len(group)\n",
    "        except AssertionError:\n",
    "            print('Assertion Error.')\n",
    "            display(group.loc[[j for j in group.index if j not in df_elev.index]])\n",
    "            break\n",
    "\n",
    "        dfoo = group.merge(df_elev, left_index=True, right_index=True,how='left')\n",
    "    dfresult = pd.concat([dfresult, dfoo])\n",
    "    #dfresult = dfresult.append(dfoo)\n",
    "\n",
    "# finally append missing station rows\n",
    "#dfresult = dfresult.append(dfresult_filled[dfresult_filled.STATION.isnull()])\n",
    "dfresult = pd.concat([dfresult, dfresult_filled[dfresult_filled.STATION.isnull()]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult2 = dfresult.merge(dftopN.loc[:,['lakecode',\n",
    "                                              'Elevation']].drop_duplicates().rename({'Elevation':'Lake_elevation'},axis=1),\n",
    "               left_on='lakecode',right_on='lakecode',validate='many_to_one',how='left')\n",
    "\n",
    "dfresult2['Elevation_difference'] = dfresult2.ELEV_GROUND/3.28084 - dfresult2.Lake_elevation\n",
    "\n",
    "dfresult2[np.abs(dfresult2.Elevation_difference) >100].drop_duplicates(['STATION','lakecode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dfresult2.drop_duplicates(['lakecode','STATION'])[['Lake_elevation', 'ELEV_GROUND']].T.values\n",
    "\n",
    "ydiff = y/3.28084 - x\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(x,ydiff,marker='.',ls='none')\n",
    "ax.set_ylabel('Station-lake elevation difference (m)')\n",
    "ax.set_xlabel('Lake elevation (m)')\n",
    "#ax.plot([100,700],[100,700],color='k',lw=0.5)\n",
    "ax.axhline(0,color='k',lw=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icemodels",
   "language": "python",
   "name": "icemodels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
