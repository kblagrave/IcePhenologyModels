{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT:**\n",
    "- topUSlakes.csv: list of top 100 US lakes\n",
    "- us_lakes_ts_minimal.csv: ice phenology data for US lakes\n",
    "- top100_extra_weather_all_deltaElev_100m.csv: meteorological time series built in `2 - Build continuous weather time series` notebook.\n",
    "\n",
    "- US_lakes_climateNA_1901-2020MP.csv: ClimateNA monthly data from 1901-2020 for each lake coordinate and elevation\n",
    "\n",
    "**OUTPUT:**\n",
    "- model_input_v10.csv OR\n",
    "- model_input_v11.csv and model_input_v11b.csv OR\n",
    "- model_input_v12.csv and model_input_v12b.csv\n",
    "\n",
    "\n",
    "**DEPENDENCIES**:\n",
    "- ``kpmb_weather.py`` is found in `modules` directory\n",
    "- ``phenology_temperature`` is found in `modules` directory\n",
    "\n",
    "**NOTEBOOK SUMMARY**\n",
    "- Smooths time series using Chebyshev polynomial fit (n=13)\n",
    "- Calculates two zero crossing dates (positive to negative temperatures and negative to positive temperatures)\n",
    "- Adjusts all meteorological time series so they are relative to occurrence of ice-on or ice-off event\n",
    "- Calculates **Freezing Degree Days** and **Positive Degree Days** in period before ice-on and ice-off (2 ways):\n",
    "    1. Look only at temperatures between zero-crossing and subsequent ice-on or ice-off\n",
    "    2. Look at all temperatures before ice-on; and all temperatures between ice-on and ice-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file was modified on September 1, 2022 to include ClimateNA solar radiation data (MJ m-2 d-1), where available.\n",
    "- calculate seasonal average (MJ m-2 d-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from numpy.polynomial import Chebyshev\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../modules')\n",
    "from kpmb_weather import get_date\n",
    "\n",
    "# for visualization of temperature record and ice on and ice off dates\n",
    "from phenology_temperature import iceon_off_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters matching those set in `2 - Build continuous weather time series` (connected to file names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlakes = 100\n",
    "delta_elev_m = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize these directories and file names\n",
    "\n",
    "Define ICEMODELS_DIR, the directory where the ice phenology data are saved.\n",
    "\n",
    "Define location of us_lakes_csv, file containing ice phenology data for all lakes. Columns must include:\n",
    "- lakecode : a unique code for the given lake\n",
    "- start_year : beginning of winter season (e.g., 1990 for winter of 1990-1991)\n",
    "- ice_on_1 : first ice on of season\n",
    "- ice_off_1, ice_off_2, ... , ice_off_6 : first, second, .. sixth ice-off event of season (if there is only one ice-off, the rest of the columns can be NaN)\n",
    "- froze : 'Y' or 'N' (whether the lake froze)\n",
    "\n",
    "Define location of climateNA_csv, file containing output from ClimateNA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = 'My Passport for Mac'\n",
    "ICEMODELS_DIR = Path(f'/Volumes/{volume}/IceModels/')\n",
    "\n",
    "us_lakes_csv = Path('us_lakes_ts_minimal.csv')\n",
    "\n",
    "climateNA_csv= Path('climateNA.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file contains the list of US lakes used in this study\n",
    "dftopN = pd.read_csv(ICEMODELS_DIR/f'topUSlakes.csv',low_memory=False)\n",
    "\n",
    "# ice phenology for all US lakes\n",
    "dfice = pd.read_csv(us_lakes_csv,low_memory=False)\n",
    "\n",
    "\n",
    "# added 'extra' weather for snow and precipitation, etc.\n",
    "#   and removed dates with \"quality\" flags\n",
    "filename = ICEMODELS_DIR/f'/top{nlakes}_extra_weather_all_deltaElev_{delta_elev_m}m.csv'\n",
    "\n",
    "dfworkingFilled = pd.read_csv(filename, low_memory=False)\n",
    "dfworkingFilled['DATE'] = pd.to_datetime(dfworkingFilled.DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional QAQC.\n",
    "Check snow on warm days (TMIN>5)... Matches original weather record and no quality flags. So keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indmin = (dfworkingFilled['SNOW'] > 0) & (dfworkingFilled['TMIN']>5)\n",
    "display(dfworkingFilled[indmin].dropna(how='all',axis=1))\n",
    "plt.plot(dfworkingFilled.loc[indmin,'TMIN'],\n",
    "         dfworkingFilled.loc[indmin,'SNOW'],ls='none',marker='o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of leap years for easier calculation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_leap_years = [i for i in range(1800,2021,4) if (\n",
    "                                                    ((i % 400)==0) | \n",
    "                                                        ((\n",
    "                                                            (i % 100)!=0) & ((i % 4)==0)))]\n",
    "#list_of_leap_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganize and filter meteorological time series\n",
    "Convert all dates to DOY, relative to Dec 31 = 0 (Dec 30 = -1, Jan 1 = 1, etc.)\n",
    "\n",
    "Pivot table to produce four tables\n",
    "- columns are DOY\n",
    "- rows are (lakecode, year)\n",
    "- values are one of TMINMAX, SNOW, PRCP or SNWD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reorganize meteorological time series\n",
    "- Organize by ice season (e.g., 2002-2003)\n",
    "- Convert dates to day of year relative to Dec 31=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine day of year\n",
    "dfworkingFilled['doy'] = dfworkingFilled.DATE.dt.day_of_year\n",
    "\n",
    "# determine start_year; i.e., the year before time series\n",
    "dfworkingFilled['start_year'] = dfworkingFilled.DATE.dt.year-1\n",
    "#  e.g., 2002-01-01 to 2002-12-31 has a start_year of 2001 at this point\n",
    "\n",
    "# convert DOY to negative if July 1st or later (182 or 183 in leap year)\n",
    "\n",
    "# find all rows that are in a leap year\n",
    "ly = (dfworkingFilled.start_year+1).isin(list_of_leap_years)\n",
    "print('Number of rows in a leap year:', ly.sum())\n",
    "ind = ((dfworkingFilled.doy>181) & ~ly) | ((dfworkingFilled.doy>182) & ly)\n",
    "\n",
    "dfworkingFilled.loc[ind,'start_year'] = dfworkingFilled.loc[ind,'start_year']+1\n",
    "# e.g., 2002-07-01 to 2002-12-31 have a start_year of 2002 (other 2002 dates have a start_year of 2001)\n",
    "\n",
    "# adjust so day of year is symmetric (both negative and positive) around Dec 31.\n",
    "#   e.g., Dec 31 is 0, Dec 30 is -1, Jan 1 is 1, Jan 2 is 2.\n",
    "dfworkingFilled.loc[ind,'doy'] = dfworkingFilled.loc[ind,'doy']-(365+ly)\n",
    "\n",
    "# Create a dictionary of meteorological dataframes\n",
    "#   Dataframes have Lake and year as row indices; and columns are day of year. (-182 to 183)\n",
    "dfdaily = {}\n",
    "for var in ['TMINMAX','SNOW','PRCP','SNWD']:\n",
    "    dfdaily[var] = dfworkingFilled.pivot_table(index=['lakecode','start_year'],\n",
    "                                               dropna=False,columns='doy',values=var)\n",
    "    print(var, dfdaily[var].shape)\n",
    "\n",
    "# Get index (i.e., lakecode and start_year) used to identify each of the 19600 rows of the dataframe\n",
    "\n",
    "index_order = dfdaily['TMINMAX'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aligning ice phenology data and meteorological data\n",
    "\n",
    "a) Calculate ice on day of year, ice off day of year, and duration\n",
    "\n",
    "b) Reorganize so ice index matches weather index.\n",
    "\n",
    "c) Select only those indices (i.e., (lakecode, start_year)) that have both an ice phenology record and a meteorological time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "# ice on is first ice on event of year\n",
    "dfice['ice_on_doy'] = (pd.to_datetime(dfice.ice_on_1) - pd.to_datetime(dfice.start_year.astype(str)+'-12-31')).dt.days\n",
    "\n",
    "# ice off is last ice off event of year\n",
    "dfice['ice_off_doy'] = (pd.to_datetime(dfice.ice_off_6.fillna(dfice.ice_off_5.fillna(dfice.ice_off_4).fillna(dfice.ice_off_3).fillna(dfice.ice_off_2).fillna(dfice.ice_off_1))) - pd.to_datetime(dfice.start_year.astype(str)+'-12-31')).dt.days\n",
    "\n",
    "# duration is difference between ice on and ice off\n",
    "dfice['ice_duration'] = dfice['ice_off_doy'] - dfice['ice_on_doy']\n",
    "\n",
    "# ice duration is 0 if the lake did not freeze\n",
    "ind_nofreeze = dfice.froze=='N'\n",
    "dfice.loc[ind_nofreeze,'ice_duration'] = 0\n",
    "\n",
    "# b)\n",
    "ice_index = dfice.set_index(['lakecode','start_year']).index\n",
    "\n",
    "# extract relevant indices (i.e., (lakecode,start_year)) from reorganized weather time series dataframe \n",
    "#   choosing only those indices that have a match in the ice phenology record\n",
    "weather_index = pd.MultiIndex.from_tuples([i for i in index_order if i in ice_index],names=['lakecode','start_year'])\n",
    "\n",
    "## c)\n",
    "# select only those ice phenology records that also have a meteorological time series\n",
    "df_lakeice = dfice.set_index(['lakecode','start_year']).loc[weather_index,:]\n",
    "\n",
    "# select only those meteorological time series that also have ice phenology records\n",
    "df_lakeweather = {}\n",
    "for var in ['TMINMAX','SNOW','PRCP','SNWD']:\n",
    "    df_lakeweather[var] = dfdaily[var].loc[weather_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QAQC.\n",
    "Look at rows that have missing TMINMAX. May not be able to use these rows for FDD and PDD calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = df_lakeweather['TMINMAX'].loc[:,-183:181].isnull().any(axis=1)\n",
    "print(ind.sum(),'out of', df_lakeweather['TMINMAX'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at rows that have some NAs, but not all NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakeweather['TMINMAX'].dropna(how='all',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find length of time series that is needed for complete ice season, once shifted to ice-on as Day Zero. (i.e., maximum duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = df_lakeice.ice_duration.max()\n",
    "print(\"Latest day (after ice on), i.e., longest duration:\",max_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find zero-crossing day based on air temperature time series (as DOY)\n",
    "- using `df_lakeweather_offset_iceon['TMINMAX']` and `df_lakeweather_offset_iceoff['TMINMAX']`\n",
    "- find day of earliest negative temperature in the entire record, **min_doy** \n",
    "- find latest ice off DOY, **max_doy**\n",
    "- calculate cumulative degree days time series starting at **min_doy** and ending at **max_doy**\n",
    "- smooth this record using Chebyshev 13th degree polynomial.\n",
    "- minimum that occurs before ice-on is the freezing zero-crossing\n",
    "- maximum that occurs before ice-off is the thawing zero-crossing\n",
    "\n",
    "\n",
    "Find zero-crossing based on DOY from original data, independent of ice-on and ice-off knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_day = (df_lakeweather['TMINMAX']<=0).any(axis=0).replace(False,np.nan).dropna().index[0]\n",
    "max_day = df_lakeice.ice_off_doy.max()\n",
    "print(min_day, max_day)\n",
    "# start a couple of weeks before min_day\n",
    "delta_days = 14\n",
    "\n",
    "tcusum = df_lakeweather['TMINMAX'].loc[:,min_day - delta_days:max_day + delta_days].cumsum(axis=1)\n",
    "print(tcusum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcusum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 7\n",
    "df_lakeweather['TMINMAX'].loc[:,min_day - delta_days:max_day + delta_days].loc[('JGL01',1997),:].plot(lw=0.5)\n",
    "df_lakeweather['TMINMAX'].loc[:,min_day - delta_days:max_day + delta_days].loc[('JGL01',1997),:].rolling(win).mean().plot()\n",
    "plt.gca().axhline(0,color='k')\n",
    "ax2 = plt.gca().twinx()\n",
    "tcusum.loc[('JGL01',1997),:].plot(ax=ax2)\n",
    "df_lakeweather['TMINMAX'].loc[:,min_day - delta_days:max_day + delta_days].loc[('JGL01',1997),:].rolling(win).mean().cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of \"problem\" lakes/years to manually confirm work OK with given algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_i = [('DMR2', 1998.0),\n",
    " ('DMR2', 2001.0),\n",
    " ('JD01', 1995.0),\n",
    " ('JD01', 1998.0),\n",
    " ('JGL01', 1997.0),\n",
    " ('JJM1', 2000.0),\n",
    " ('JJM1', 2010.0),\n",
    " ('JJM1', 2012.0),\n",
    " ('JJM1', 2017.0),\n",
    " ('JJM18', 1998.0),\n",
    " ('JJM18', 2003.0),\n",
    " ('JJM2', 2010.0),\n",
    " ('JJM2', 2012.0),\n",
    " ('JJM2', 2017.0),\n",
    " ('JJM27', 1998.0),\n",
    " ('JJM28', 1981.0),\n",
    " ('JJM4', 2010.0),\n",
    " ('JJM4', 2012.0),\n",
    " ('JJM4', 2017.0),\n",
    " ('JJM6', 2010.0),\n",
    " ('JJM6', 2012.0),\n",
    " ('JJM6', 2017.0),\n",
    " ('JJM9', 1995.0),\n",
    " ('KMS14', 1992.0),\n",
    " ('KMS19', 1973.0),\n",
    " ('KMS25', 1976.0),\n",
    " ('KMS25', 1997.0),\n",
    " ('KMS25', 2003.0),\n",
    " ('LR1', 2005.0),\n",
    " ('LR2', 2001.0),\n",
    " ('MICH03', 1988.0),\n",
    " ('MICH03', 1989.0),\n",
    " ('MICH03', 2005.0),\n",
    " ('MICH03', 2011.0),\n",
    " ('MICH06', 2008.0),\n",
    " ('MINN34', 1987.0),\n",
    " ('MINN4', 2001.0),\n",
    " ('xKB0014', 1994.0),\n",
    " ('xKB0019', 1990.0),\n",
    " ('xKB0263', 1980.0),\n",
    " ('xKB0269', 1990.0),\n",
    " ('xKB0269', 1997.0),\n",
    " ('xKB0269', 2011.0),\n",
    " ('xKB0364', 1977.0),\n",
    " ('xKB1162', 1997.0),\n",
    " ('xKB1370', 1991.0),\n",
    " ('xKB1746', 1986.0),\n",
    " ('xKB1746', 1998.0),\n",
    " ('xKB1921', 1981.0),\n",
    " ('xKB1921', 1998.0),\n",
    " ('xKB1921', 1980.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev_smooth = False\n",
    "window = 7 # days \n",
    "\n",
    "df_zero_cross = pd.DataFrame(index = df_lakeweather['TMINMAX'].index)\n",
    "\n",
    "dfsmoothed_tminmax = pd.DataFrame(index = df_lakeweather['TMINMAX'].index)\n",
    "# degree of polynomial fit to cumulative sum of temperature time series\n",
    "ndeg = 13\n",
    "ii =0\n",
    "for i,row in tcusum.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    ii+=1\n",
    "    print(ii)\n",
    "    x = row.index.astype(float)\n",
    "    y = row.astype(float).values\n",
    "    ind = row.isnull()\n",
    "    x = x[~ind]\n",
    "    y = y[~ind]\n",
    "    if len(x)==0:\n",
    "        continue\n",
    "        \n",
    "    ice_on_doy = df_lakeice.loc[i,'ice_on_doy']\n",
    "    ice_off_doy = df_lakeice.loc[i,'ice_off_doy']\n",
    "    \n",
    "    # find zero-crossings based on different rolling mean temperatures \n",
    "    for win in [1,3,5,7,14]:\n",
    "        newx = np.arange(x[0],x[-1])\n",
    "        # first make sure series in continuous\n",
    "        newy = np.interp(newx, x, y)\n",
    "        newy = pd.Series(newy,index=newx).rolling(win).mean()\n",
    "        \n",
    "        # find peaks (maxima);\n",
    "        peaks = newx[find_peaks(newy)[0]]\n",
    "\n",
    "        # find troughs (minima); \n",
    "        troughs = newx[find_peaks(-newy)[0]]\n",
    "\n",
    "        # choose first peak; i.e. first time mean temp crosses below zero\n",
    "        if len(peaks)>0:\n",
    "            freeze_doy = peaks[0]\n",
    "        else:\n",
    "            freeze_doy = np.nan\n",
    "\n",
    "        # choose first trough, but must be after ice_on_doy and after freeze_doy\n",
    "    \n",
    "        troughs = [t for t in troughs if (t > ice_on_doy) & (t > freeze_doy)]\n",
    "        \n",
    "        if len(troughs)>0:\n",
    "            thaw_doy = troughs[0]\n",
    "        else:\n",
    "            thaw_doy = np.nan\n",
    "        df_zero_cross.loc[i,f'ZC{win}FreezeDOY'] =freeze_doy\n",
    "        df_zero_cross.loc[i,f'ZC{win}ThawDOY'] = thaw_doy\n",
    "        \n",
    "    c = Chebyshev.fit(x,y,deg=ndeg)\n",
    "    newx = np.linspace(x[0],x[-1],1000)\n",
    "    newy = c(newx)\n",
    "\n",
    "    # save Chebyshev smoothed time series to new dataframe\n",
    "    dfsmoothed_tminmax.loc[i,range(int(x[0]),int(x[-1])+1)] = c(range(int(x[0]),int(x[-1])+1))\n",
    "    \n",
    "    # find peaks (maxima)\n",
    "    peaks = newx[find_peaks(newy)[0]]\n",
    "    \n",
    "    # find troughs (minima)\n",
    "    troughs = newx[find_peaks(-newy)[0]]\n",
    "    \n",
    "    # choose first peak\n",
    "    if len(peaks)>0:\n",
    "        freeze_doy = peaks[0]\n",
    "    else:\n",
    "        freeze_doy = np.nan\n",
    "    \n",
    "    # choose last trough\n",
    "    if len(troughs)>0:\n",
    "        thaw_doy = troughs[-1]\n",
    "    else:\n",
    "        thaw_doy = np.nan\n",
    "        \n",
    "    df_zero_cross.loc[i,'ZCFreezeDOY'] =freeze_doy\n",
    "    df_zero_cross.loc[i,'ZCThawDOY'] = thaw_doy\n",
    "    \n",
    "    if i in check_i:\n",
    "        continue\n",
    "        tmp_ts = df_lakeweather['TMINMAX'].loc[:,min_day - delta_days:max_day + delta_days].loc[i,:]\n",
    "        print(i)\n",
    "        plt.plot(x,y)\n",
    "        plt.plot(newx,newy)\n",
    "        plt.axvline(freeze_doy,color='C0',lw=2)\n",
    "        plt.axvline(thaw_doy,color='C3',lw=2)\n",
    "        yy = np.max(plt.gca().get_ylim())\n",
    "        for iii,xx in df_zero_cross.loc[i,:].iteritems():\n",
    "            if 'Thaw' in iii:\n",
    "                color='C3'\n",
    "            else:\n",
    "                color='C0'\n",
    "            plt.axvline(xx,ls=':',c=color)\n",
    "            plt.gca().text(xx,yy, ''.join(re.findall('\\d+',iii)),ha='center',rotation=0,fontsize=20,color=color)\n",
    "        plt.axvline(ice_on_doy,lw=8,color='C0',alpha=0.5)\n",
    "        plt.axvline(ice_off_doy,lw=8,color='C3',alpha=0.5)\n",
    "        ax = plt.gca()\n",
    "        ax.set_ylabel('Cumulative degree days')\n",
    "        ax2 = ax.twinx()\n",
    "        tmp_ts.plot(ax=ax2,color='k',lw=0.5)\n",
    "        ax2.axhline(0,lw=0.5,color='k')\n",
    "        ax2.set_ylabel('Temperature')\n",
    "        plt.show()\n",
    "        display(df_zero_cross.loc[i,:])\n",
    "        input('continue?')\n",
    "        \n",
    "    #if not row.isnull().all():\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DOY = df_zero_cross.merge(df_lakeice[['lake','froze','ice_on_doy','ice_off_doy','ice_duration']],\n",
    "                    validate='one_to_one',\n",
    "                    how='outer',left_index=True,right_index=True)\n",
    "display(df_DOY.head())\n",
    "print(df_DOY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Pearson r coefficients of ice_on_doy and all ZC?FreezeDOY columns. Which offers the tightest correlation?\n",
    "\n",
    "Look at Pearson r coefficients of ice_off_doy and all ZC?ThawDOY columns. Which offers the tightest correlation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame(columns=['r','p'])\n",
    "\n",
    "for cc in [cc for cc in df_DOY.columns if ('Thaw' in cc) | ('Freeze' in cc)]:\n",
    "    x = df_DOY[cc].copy()\n",
    "    y = df_DOY['ice_on_doy'].copy()\n",
    "    if 'Thaw' in cc:\n",
    "        y = df_DOY['ice_off_doy'].copy()\n",
    "    ind = x.isnull() | y.isnull()\n",
    "    x = x[~ind]\n",
    "    y = y[~ind]\n",
    "    r,p = pearsonr(x,y)\n",
    "    df_corr.loc[cc,'r'] = r\n",
    "    df_corr.loc[cc,'p'] = p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.sort_values('r',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best correlation is between ice-on/off and the zero-crossing day determined from polynomial fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of zero-crossing and ice-on/ice-off delay\n",
    "- negative delay values could be a result of:\n",
    "    - local air temperatures being different than those reported at the nearest weather station\n",
    "    - zero-crossing is looking at the general trend of temperature\n",
    "        - there could still be negative (or positive) temperatures preceding zero-crossing\n",
    "    - wind events prior to ice off could result in ice breakup, even when temperatures are hovering slightly below zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-50,125,80)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "(df_DOY.ice_on_doy - df_DOY.ZCFreezeDOY).hist(ax=ax,bins=bins,label='Ice On delay')\n",
    "(df_DOY.ice_off_doy- df_DOY.ZCThawDOY).hist(ax=ax,alpha=0.5,bins=bins, label=\"Ice Off delay\")\n",
    "ax.legend()\n",
    "ax.set_xlabel('Time since zero-crossing (days)')\n",
    "ax.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDD and PDD: Antecedent conditions\n",
    "Reorganize so all weather variables are relative to ice-on or ice-off date, removing weather data from rows with no ice-on or ice-off information.\n",
    "\n",
    "We will keep 366 days. \n",
    "\n",
    "ICEON:\n",
    "- 210 days after ice-on (to cover all ice durations)\n",
    "- and thus 155 days before ice-on.\n",
    "\n",
    "ICEOFF:\n",
    "- 10 days after ice-on\n",
    "- 355 days before ice-off\n",
    "\n",
    "\n",
    "#### 1. Shift meteorological time series using the ice_on_doy column in df_lakeice.\n",
    "\n",
    "Need to add columns. Otherwise shifting would shift some values out of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_day_column = 210\n",
    "min_day_column = -355\n",
    "#min_day_column = -183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakeweather_offset_iceon = {}\n",
    "df_lakeweather_offset_iceoff = {}\n",
    "\n",
    "\n",
    "# create dataframe with additional columns in it (-355 to -184 and 183 to 210)\n",
    "dftmp = pd.DataFrame(index = df_lakeweather['TMINMAX'].index, \n",
    "                     columns = \n",
    "                     list(np.arange(-184,min_day_column-1,-1))+\n",
    "                     list(np.arange(183,max_day_column+1)))\n",
    "\n",
    "\n",
    "for var in ['TMINMAX','SNOW','PRCP','SNWD']:\n",
    "    \n",
    "    dfworking = df_lakeweather[var].copy()\n",
    "    \n",
    "    # add additional columns\n",
    "    dfworking = pd.concat([dfworking,dftmp],axis=1) \n",
    "    \n",
    "    # sort columns\n",
    "    dfworking = dfworking.loc[:,\n",
    "                list(np.arange(min_day_column,max_day_column+1))].copy()\n",
    "\n",
    "    # start with complete meteorological time series\n",
    "    dfon_ = dfworking.copy()\n",
    "    dfoff_ = dfworking.copy()\n",
    "    \n",
    "    for ii in range(-183,183):\n",
    "        # Find all lakes/years that have ice-on on day -ii\n",
    "        ind1 = df_lakeice.ice_on_doy == -ii\n",
    "        \n",
    "        # Find all lakes/years that have ice-off on day -ii\n",
    "        ind2 = df_lakeice.ice_off_doy == -ii\n",
    "        \n",
    "        # continue if there are no lakes/years with ice on or ice off on day -ii\n",
    "        if (ind1.sum()==0) & (ind2.sum()==0):\n",
    "            continue        \n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(var, ii)\n",
    "        \n",
    "        # shift the entire time series so it will be relative to ice on date\n",
    "        #df_shift = df_lakeweather[var].loc[:,-183:182].shift(ii,axis=1).copy()\n",
    "        df_shift = dfworking.loc[:,:].shift(ii,axis=1).copy()\n",
    "        \n",
    "        # replace timeseries with values from shifted dataframe\n",
    "        # select only those rows that actually have lakes/years with ice-on on day -ii\n",
    "        dfon_.loc[ind1,:] = df_shift[ind1].copy()\n",
    "        # select only those rows that actually have lakes/years with ice-off on day -ii\n",
    "        dfoff_.loc[ind2,:] = df_shift[ind2].copy()\n",
    "        \n",
    "        \n",
    "    # go back and remove weather info if there is no info for ice_on_doy or ice_off_doy\n",
    "    # df_lakeice and df_lakeweather dataframes have the same row indices\n",
    "    ind1 = df_lakeice.ice_on_doy.isnull()\n",
    "    ind2 = df_lakeice.ice_off_doy.isnull()\n",
    "    #dfon_.loc[ind1,-183:182] = np.nan\n",
    "    #dfoff_.loc[ind2,-183:182] = np.nan\n",
    "    \n",
    "    dfon_.loc[ind1,:] = np.nan\n",
    "    dfoff_.loc[ind2,:] = np.nan\n",
    "    \n",
    "    # write these dataframes to their respective dictionary fields\n",
    "    df_lakeweather_offset_iceon[var] = dfon_\n",
    "    df_lakeweather_offset_iceoff[var] = dfoff_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_zero_cross.dropna().ZCFreeze.hist(bins = np.linspace(-130,30,80),alpha=0.5)\n",
    "#df_zero_cross.dropna().ZCThaw.hist()\n",
    "#(df_zero_cross.dropna().ZCThaw - df_zero_cross.dropna().IceDuration).hist(bins = np.linspace(-130,30,80),alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FDD and PDD in two ways\n",
    "1. Use zero-crossing date to calculate a FDD and PDD prior to ice-on and ice-off respectively; only include days after zero-crossing\n",
    "\n",
    "2. Calculate total FDD prior to ice-on; and total PDD between ice-on and ice-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_zero_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DOY.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_lakeweather_offset_iceon['TMINMAX'].copy() # this is entire time series shifted so iceon is Day Zero\n",
    "\n",
    "x0 = (df_temp<0).any(axis=0).replace(False,np.nan).dropna().index[0]\n",
    "print(x0)\n",
    "\n",
    "for i,row in df_DOY.iterrows():\n",
    "    if np.isnan(row.ice_duration) | (row.ice_duration==0):\n",
    "        continue\n",
    "    \n",
    "    #  1. FDD and PDD based on zero-crossing\n",
    "    #\n",
    "    if np.isnan(row.ZCFreezeDOY):\n",
    "        fdd = np.nan\n",
    "    else:\n",
    "        zc = int(round(row.ZCFreezeDOY - row.ice_on_doy))\n",
    "        if zc > 0:\n",
    "            zc = 0\n",
    "        pre_iceon = df_temp.loc[i,zc:0]\n",
    "        fdd = np.abs(pre_iceon[pre_iceon<0].sum())\n",
    "    if np.isnan(row.ZCThawDOY):\n",
    "        pdd = np.nan\n",
    "    else:\n",
    "        zc = int(round(row.ZCThawDOY - row.ice_on_doy))\n",
    "        if zc > row.ice_duration:\n",
    "            zc = row.ice_duration\n",
    "        pre_iceoff = df_temp.loc[i,zc:row.ice_duration]\n",
    "        pdd = pre_iceoff[pre_iceoff>0].sum()\n",
    "    \n",
    "    #  2. TOTAL FDD and PDD\n",
    "    # look at all days before ice on; x0 is -140, earliest below zero day\n",
    "    pre_iceon2 = df_temp.loc[i,x0:0]\n",
    "    fdd2 = np.abs(pre_iceon2[pre_iceon2<0].sum())\n",
    "    \n",
    "    # look at all days between ice on and ice off\n",
    "    pre_iceoff2 = df_temp.loc[i,0:row.ice_duration]\n",
    "    pdd2 = pre_iceoff2[pre_iceoff2>0].sum()\n",
    "    \n",
    "    df_DOY.loc[i,'FDD_ZC'] = fdd\n",
    "    df_DOY.loc[i,'PDD_ZC'] = pdd\n",
    "    \n",
    "    df_DOY.loc[i,'FDD'] = fdd2\n",
    "    df_DOY.loc[i,'PDD'] = pdd2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the earliest day that temp drops below 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Earliest day (before ice on):\",\n",
    "      (df_temp.iloc[:,2:]<0).any(axis=0).replace(False,np.nan).dropna().index[0])\n",
    "print(\"Latest day (after ice on), i.e., longest duration:\",df_lakeice.ice_duration.max())\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display example temperature and ice phenology record\n",
    "\n",
    "- Look for years and lakes where PDD and FDD are zero (from 4a - Data exploration notebook)\n",
    "- There are 51 total (22 FDD are zero, 29 PDD are zero)\n",
    "- Display these here to see what is going on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsmoothed_tminmax = dfsmoothed_tminmax.loc[:,dfsmoothed_tminmax.columns.sort_values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lkcode, year = 'DMR2', 2003\n",
    "#lkcode, year = 'xKB1045',1991\n",
    "for lkcode,year in zip(['DMR1','DMR2'],[2003,2003]):\n",
    "    #lakename = dfice.loc[dfice.lakecode==lkcode,'lake'].drop_duplicates().values[0].title()\n",
    "    duration = dfice[(dfice.lakecode==lkcode) & (dfice.start_year==year)].ice_duration.values[0]\n",
    "    if np.isnan(duration):\n",
    "        continue\n",
    "    iceon_off_summary(lkcode, year, df_lakeweather_offset_iceon, dfsmoothed_tminmax,\n",
    "                      df_DOY, zc = True, date_range=(-140, 198))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal weather patterns\n",
    "- Calculate seasonal weather patterns from daily NOAA meteorological time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfworkingFilled.loc[(dfworkingFilled.lakecode=='DMR1')& (dfworkingFilled.start_year==1852),'SNOW'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfworkingFilled['Year'] = dfworkingFilled.DATE.dt.year\n",
    "dfworkingFilled['Month'] = dfworkingFilled.DATE.dt.month\n",
    "dfworkingFilled['Year_lag1'] = dfworkingFilled.Year-1\n",
    "\n",
    "ind = dfworkingFilled.Month<6\n",
    "dfworkingFilled.loc[ind,'WinterYear'] = dfworkingFilled.loc[ind,'Year_lag1']\n",
    "dfworkingFilled.loc[~ind,'WinterYear'] = dfworkingFilled.loc[~ind,'Year']\n",
    "dfworkingFilled['Season'] = ((dfworkingFilled.Month+6) // 3) % 4\n",
    "\n",
    "complete_seasons = dfworkingFilled.groupby(['lakecode','WinterYear','Season']).Month.apply(lambda x: len(x.unique())==3).reset_index().rename({'Month':'CompleteSeason'},axis=1)\n",
    "\n",
    "print('Status: first merger')\n",
    "dfworkingFilled_extra = dfworkingFilled.merge(complete_seasons,left_on=['lakecode','WinterYear','Season'],\n",
    "                      right_on=['lakecode','WinterYear','Season'],validate='many_to_one',how='left')\n",
    "\n",
    "print('Status: entering for loop')\n",
    "for var in ['TMINMAX','SNOW','SNWD','PRCP']:\n",
    "    dfworkingFilled_extra.loc[~dfworkingFilled_extra.CompleteSeason,var] = np.nan\n",
    "\n",
    "    season_dict = {0:f'{var}_lagJJA',1:f'{var}_lagSON',2:f'{var}_DJF',3:f'{var}_MAM'}\n",
    "    print(var)\n",
    "    if var in ['TMINMAX','SNWD']:\n",
    "        dfseasonal_ = dfworkingFilled_extra.groupby(['lakecode','WinterYear','Season'])[var].mean().reset_index().pivot_table(index = ['lakecode','WinterYear'],columns='Season',values=var).rename(season_dict,axis=1)\n",
    "    else:\n",
    "        dfseasonal_ = dfworkingFilled_extra.groupby(['lakecode','WinterYear','Season'])[var].sum(min_count=1).reset_index().pivot_table(index = ['lakecode','WinterYear'],columns='Season',values=var).rename(season_dict,axis=1)\n",
    "    if var=='TMINMAX':\n",
    "        dfseasonal_all = dfseasonal_.copy()\n",
    "    else:\n",
    "        dfseasonal_all = dfseasonal_all.merge(dfseasonal_,left_index=True,right_index=True,how='outer',validate='one_to_one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfresult2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**26 APRIL 2023 UPDATE**\n",
    "- added fdd_zc and hdd_zc columns\n",
    "\n",
    "What about zero duration no freeze seasons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_DOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dftopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfseasonal_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = dfseasonal_all.merge(df_DOY.drop('lake',axis=1).rename_axis(('lakecode','WinterYear')).rename(\n",
    "    {'FDD':'FDD_on','PDD':'PDD_off','FDD_ZC':'FDD_on_zc','PDD_ZC':'PDD_off_zc'},axis=1),\n",
    "                                left_index=True,right_index=True, how='outer')\n",
    "\n",
    "#df_final = dfseasonal_all.merge(df_fdd.rename_axis(('lakecode','WinterYear')).rename('FDD_on'),\n",
    "#                                left_index=True,right_index=True,how='outer').merge(\n",
    "#                                df_hdd.rename_axis(('lakecode','WinterYear')).rename('HDD_off'),\n",
    "#                                left_index=True,right_index=True,how='outer').merge(\n",
    "#                                df_fdd_smooth.rename_axis(('lakecode','WinterYear')).rename('FDD_on_smooth'),\n",
    "#                                left_index=True,right_index=True,how='outer').merge(\n",
    "#                                df_hdd_smooth.rename_axis(('lakecode','WinterYear')).rename('HDD_off_smooth'),\n",
    "#                                left_index=True,right_index=True,how='outer')\n",
    "\n",
    "display(df_final.columns)\n",
    "\n",
    "df_final = df_final.join(dftopN.set_index('lakecode'),how='outer').rename_axis(('lakecode','start_year')).reset_index()\n",
    "\n",
    "#df_final = df_final.merge(dfice[['lakecode','start_year','ice_on_doy','ice_off_doy','ice_duration']], left_index=True,\n",
    "#               right_on= ['lakecode','start_year'],how='left')\n",
    "first_columns= ['lakecode','lake','start_year','ice_on_doy','ice_off_doy','ice_duration','lat','lon']\n",
    "next_columns = [c for c in df_final.columns if c not in first_columns]\n",
    "df_final = df_final[first_columns+next_columns].drop(['start_date','end_date'],axis=1).sort_values(\n",
    "    ['lakecode','start_year']).reset_index(drop=True)\n",
    "df_final.columns\n",
    "#df_final.HDD_off.hist()\n",
    "df_final.PDD_off.hist() # blue\n",
    "df_final.PDD_off_zc.hist(alpha=0.5) # orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 10 excludes HydroLAKES match for Peltier Lake, MN\n",
    "\n",
    "Version 11 is completely rebuilt weather time series based on TMINMAX values and including snow and precipitation\n",
    "- changed column headings\n",
    "    - level_0 -> lakecode\n",
    "    - GDD_off -> HDD_off\n",
    "    - duration -> ice_duration\n",
    "- new column headings\n",
    "    - SNOW seasonal (total snow fall)\n",
    "    - SNWD seasonal (average snow depth)\n",
    "    - PRCP seasonal (total precipitation)\n",
    "- removed column headings\n",
    "    - GDD_on\n",
    "    - FDD_off\n",
    "    - FDD_offseason\n",
    "    - GDD_off\n",
    "    - FDD_year\n",
    "    - start_date\n",
    "    - end_date\n",
    "    \n",
    "Version 12 adds:\n",
    "- new column headings\n",
    "    - FDD_on_zc (FDD calculation only since zero-crossing)\n",
    "    - PDD_off_zc (PDD calculation only since zero-crossing)\n",
    "    - HDD_off renamed to PDD_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to v12 on 28 APRIL 2023\n",
    "df_final.to_csv(ICEMODELS_DIR/f'model_input_v12.csv')\n",
    "\n",
    "#df_lakeweather['SNOW'].loc[('LR1',1895),:].replace(0,np.nan).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ClimateNA solar radiation data based on lat,lon and Elevation columns\n",
    "- lagJJA, lagSON, DJF and MAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(ICEMODELS_DIR/f'model_input_v12.csv')\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclimatena = pd.read_csv(climateNA_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at ClimateNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclimatena[dfclimatena.Rad05!=-9999].Rad05.hist()\n",
    "dfclimatena.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclimatena[ (dfclimatena.Rad05!=-9999)].plot.scatter('Rad04','Tmax04',\n",
    "                                                                                               marker='.'\n",
    "                                                                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display solar radiation to confirm it makes sense. I.e., it should be spatially correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclimatena[(dfclimatena.Rad07!=-9999) & \n",
    "            (dfclimatena.Year.isin([1990]))].plot.scatter(\n",
    "    x='Longitude',y='Latitude',c='Rad07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfclimatena[(dfclimatena.Rad01 < 0) & (dfclimatena.Rad01!=-9999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at variation in solar radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax  = dfclimatena.loc[(dfclimatena.Rad05!=-9999) & dfclimatena.ID2.isin(['DMR1']),:].plot('Year','Rad03',label='DMR1')\n",
    "dfclimatena.loc[(dfclimatena.Rad05!=-9999) & dfclimatena.ID2.isin(['MINN25']),:].plot('Year','Rad03',ax=ax,label='MINN25')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ClimateNA data.\n",
    "- Shift so \"year\" runs from June-Dec then Jan-May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(ICEMODELS_DIR/f'model_input_v12.csv',index_col= 0)\n",
    "\n",
    "dfclimatena = pd.read_csv(climateNA_csv).replace(-9999,np.nan)\n",
    "dfclimatena = dfclimatena[['Year','ID2']+[f'Tave{i:02d}' for i in range(1,13)]+\n",
    "                          [f'PPT{i:02d}' for i in range(1,13)]+\n",
    "                         [f'Rad{i:02d}' for i in range(1,13)]]\n",
    "dfclimatena = dfclimatena.set_index(['ID2','Year']).stack().reset_index()\n",
    "\n",
    "dfclimatena['Month'] = dfclimatena.level_2.str[-2:].astype(int)\n",
    "\n",
    "dfclimatena['Season'] = dfclimatena['Year'].apply(lambda x: f\"{x}-{x+1}\")\n",
    "ind = dfclimatena['Month'] < 6\n",
    "dfclimatena.loc[ind,'Season'] = dfclimatena.loc[ind,'Year'].apply(lambda x: f\"{x-1}-{x}\")\n",
    "dfclimate = dfclimatena.pivot_table(index = ['ID2','Season'],columns='level_2',values=0)[[f\"PPT{i:02d}\" for i in\n",
    "                                                                              [r for r in range(6,13)]+[r for r in range(1,6)]]+\n",
    "                                                                             [f\"Tave{i:02d}\" for i in\n",
    "                                                                              [r for r in range(6,13)]+[r for r in range(1,6)]]+\n",
    "                                                                             [f\"Rad{i:02d}\" for  i in  \n",
    "                                                                              [r for r in range(6,13)]+[r for r in range(1,6)]]]\n",
    "                                                                                        \n",
    "dfclimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClimateNA (continued)\n",
    "Create seasonal temperature, precipitation and solar radiation columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new, m in {'lagJJA':[6,7,8],'lagSON':[9,10,11],'DJF':[12,1,2],'MAM':[3,4,5]}.items():\n",
    "    for v in ['PPT','Tave','Rad']:\n",
    "        columns = [f'{v}{mm:02d}' for mm in m]\n",
    "        newv  = f'{v}_{new}'\n",
    "        if v in ['Tave','Rad']:\n",
    "            dfclimate[newv] = dfclimate[columns].mean(axis=1,skipna=False)\n",
    "        else:\n",
    "            dfclimate[newv] = dfclimate[columns].sum(axis=1,skipna=False)\n",
    "            \n",
    "dfclimate['Year'] = dfclimate.index.get_level_values(1).str[:4].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2 = df_final.merge(dfclimate.iloc[:,-12:].reset_index(),\n",
    "                           right_on=['ID2','Year'],left_on=['lakecode','start_year'],how='left').drop(['ID2','Year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.to_csv(ICEMODELS_DIR/f'model_input_v12b.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that this new version (v12b) has the same indices and columns as the previous version (v11b)\n",
    "\n",
    "New columns in v12b:\n",
    "- 'ZCFreezeDOY', 'ZCThawDOY', 'froze', 'FDD_on_zc', 'PDD_off_zc', 'PDD_off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(ICEMODELS_DIR/f'model_input_v11b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = dd.set_index(['lakecode','start_year']).index\n",
    "index2 = df_final2.set_index(['lakecode','start_year']).index\n",
    "missing_index = [i for i in index1\n",
    " if i not in index2]\n",
    "display(dd.set_index(['lakecode','start_year']).loc[missing_index,:])\n",
    "missing_index = [i for i in index2\n",
    " if i not in index1]\n",
    "display(df_final2.set_index(['lakecode','start_year']).loc[missing_index,:])\n",
    "\n",
    "print([c for c in df_final2.columns if c not in dd.columns])\n",
    "print([c for c in dd.columns if c not in df_final2.columns])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icemodels",
   "language": "python",
   "name": "icemodels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
